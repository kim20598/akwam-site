import aiohttp
from bs4 import BeautifulSoup
from typing import List, Dict, Any, Optional
import re
import asyncio
from urllib.parse import urljoin, urlencode
import json

class AkwamScraper:
    def __init__(self):
        self.base_url = "https://ak.sv"
        self.session = None
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "ar,en-US;q=0.7,en;q=0.3",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1"
        }
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(headers=self.headers)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def fetch_page(self, url: str) -> Optional[str]:
        """Fetch HTML content from URL"""
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    return await response.text()
        except Exception as e:
            print(f"Error fetching {url}: {e}")
        return None
    
    def parse_home_page(self, html: str) -> Dict[str, Any]:
        """Parse home page content"""
        soup = BeautifulSoup(html, 'html.parser')
        sections = []
        
        # Extract sections (similar to original plugin logic)
        sections_data = [
            ("$mainUrl/movies", "أحدث الأفلام"),
            ("$mainUrl/series", "أحدث المسلسلات"),
            ("$mainUrl/shows", "العروض"),
        ]
        
        for section_url, title in sections_data:
            section_items = []
            section_elements = soup.select('div.container section')
            
            for element in section_elements:
                items = self.extract_items_from_section(element)
                if items:
                    section_items.extend(items)
            
            if section_items:
                sections.append({
                    "title": title,
                    "items": section_items[:12]  # Limit to 12 items per section
                })
        
        return {"sections": sections}
    
    def extract_items_from_section(self, section_element) -> List[Dict[str, Any]]:
        """Extract items from a section element"""
        items = []
        for item in section_element.select('div.col-lg-auto.col-md-4.col-6'):
            try:
                title_elem = item.select_one('h3.entry-title a')
                if not title_elem:
                    continue
                
                title = title_elem.text.strip()
                url = title_elem.get('href', '')
                
                if not url.startswith('http'):
                    url = urljoin(self.base_url, url)
                
                poster_elem = item.select_one('img')
                poster = None
                if poster_elem:
                    poster = poster_elem.get('data-src') or poster_elem.get('src')
                    if poster and not poster.startswith('http'):
                        poster = urljoin(self.base_url, poster)
                
                # Try to determine type from URL or context
                content_type = "movie"
                if "/series/" in url:
                    content_type = "series"
                elif "/show/" in url:
                    content_type = "show"
                
                items.append({
                    "id": self.extract_id_from_url(url),
                    "title": title,
                    "url": url,
                    "poster": poster,
                    "type": content_type
                })
            except Exception as e:
                continue
        
        return items
    
    def extract_id_from_url(self, url: str) -> str:
        """Extract unique ID from URL"""
        # Extract last part of URL as ID
        match = re.search(r'/([^/]+)/?$', url)
        if match:
            return match.group(1).replace('/', '')
        return url.split('/')[-1]
    
    async def search(self, query: str, page: int = 1) -> Dict[str, Any]:
        """Search for content"""
        encoded_query = urlencode({"q": query})
        url = f"{self.base_url}/search?{encoded_query}&page={page}"
        
        html = await self.fetch_page(url)
        if not html:
            return {"results": [], "total_pages": 0}
        
        soup = BeautifulSoup(html, 'html.parser')
        results = self.extract_items_from_section(soup)
        
        # Try to find pagination
        total_pages = 1
        pagination = soup.select('ul.pagination li a')
        if pagination:
            page_numbers = []
            for page_elem in pagination:
                try:
                    page_num = int(page_elem.text.strip())
                    page_numbers.append(page_num)
                except:
                    pass
            if page_numbers:
                total_pages = max(page_numbers)
        
        return {
            "results": results,
            "total_pages": total_pages,
            "current_page": page
        }
    
    async def get_content_details(self, url: str) -> Optional[Dict[str, Any]]:
        """Get detailed information about content"""
        html = await self.fetch_page(url)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # Extract title
        title_elem = soup.select_one('h1.entry-title')
        title = title_elem.text.strip() if title_elem else "Unknown"
        
        # Extract description
        description_elem = soup.select_one('h2:contains("قصة المسلسل") + div > p') or \
                          soup.select_one('meta[name="description"]')
        description = ""
        if description_elem:
            if description_elem.name == 'meta':
                description = description_elem.get('content', '').strip()
            else:
                description = description_elem.text.strip()
        
        # Extract year
        year = None
        year_elem = soup.select('div.font-size-16.text-white a[href*="/year/"]')
        if year_elem:
            try:
                year_text = year_elem[0].text.strip()
                year = int(re.search(r'\d{4}', year_text).group())
            except:
                pass
        
        # Extract poster
        poster = None
        poster_elem = soup.select_one('img[src*="poster"], img[data-src*="poster"]')
        if poster_elem:
            poster = poster_elem.get('data-src') or poster_elem.get('src')
            if poster and not poster.startswith('http'):
                poster = urljoin(self.base_url, poster)
        
        # Extract episodes if it's a series
        episodes = []
        episode_elements = soup.select('div#series-episodes div[class*="col-"]')
        for ep_elem in episode_elements:
            ep_link = ep_elem.select_one('a[href*="/episode/"]')
            if ep_link:
                ep_url = ep_link.get('href', '')
                if not ep_url.startswith('http'):
                    ep_url = urljoin(self.base_url, ep_url)
                
                ep_title = ep_link.select_one('h2')
                if ep_title:
                    ep_name = ep_title.text.strip()
                else:
                    ep_name = ep_link.text.strip()
                
                # Extract episode and season numbers
                ep_num = self.extract_episode_number(ep_name)
                season_num = 1  # Default to season 1
                
                episodes.append({
                    "id": self.extract_id_from_url(ep_url),
                    "title": ep_name,
                    "episode_number": ep_num,
                    "season_number": season_num,
                    "url": ep_url
                })
        
        # Determine content type
        content_type = "series" if episodes else "movie"
        if "/series/" in url:
            content_type = "series"
        elif "/movie/" in url:
            content_type = "movie"
        elif "/show/" in url:
            content_type = "show"
        
        return {
            "id": self.extract_id_from_url(url),
            "title": title,
            "description": description,
            "year": year,
            "poster": poster,
            "type": content_type,
            "episodes": episodes,
            "url": url
        }
    
    def extract_episode_number(self, episode_name: str) -> Optional[int]:
        """Extract episode number from episode name"""
        match = re.search(r'الحلقة\s*(\d+)', episode_name)
        if match:
            return int(match.group(1))
        
        match = re.search(r'Episode\s*(\d+)', episode_name, re.IGNORECASE)
        if match:
            return int(match.group(1))
        
        match = re.search(r'EP\s*(\d+)', episode_name, re.IGNORECASE)
        if match:
            return int(match.group(1))
        
        return None
    
    async def extract_video_sources(self, episode_url: str) -> List[Dict[str, Any]]:
        """Extract video sources from episode page"""
        html = await self.fetch_page(episode_url)
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        sources = []
        
        # Look for video sources in different formats
        video_elements = soup.select('source[src], video source[src]')
        for video_elem in video_elements:
            video_url = video_elem.get('src', '')
            if video_url:
                quality = video_elem.get('size', '') or video_elem.get('label', '') or 'SD'
                
                sources.append({
                    "url": urljoin(self.base_url, video_url),
                    "quality": quality,
                    "type": "video/mp4",
                    "size": self.get_video_size_label(quality)
                })
        
        # Also check for iframe embeds
        iframe_elements = soup.select('iframe[src]')
        for iframe in iframe_elements:
            src = iframe.get('src', '')
            if src:
                sources.append({
                    "url": src,
                    "quality": "HD",
                    "type": "embed",
                    "size": "Unknown"
                })
        
        return sources
    
    def get_video_size_label(self, quality_str: str) -> str:
        """Convert quality string to size label"""
        quality_str = str(quality_str).upper()
        if '1080' in quality_str or 'FHD' in quality_str:
            return '1080p'
        elif '720' in quality_str or 'HD' in quality_str:
            return '720p'
        elif '480' in quality_str or 'SD' in quality_str:
            return '480p'
        elif '360' in quality_str:
            return '360p'
        else:
            return 'SD'